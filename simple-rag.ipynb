{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3acf8018",
   "metadata": {},
   "source": [
    "This is an implementation of Simple \"dumb\" RAG. It just sees if the query is relevant to whats in the docs, otherwise it says \"idk\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab6e8a",
   "metadata": {},
   "source": [
    "Overview\n",
    "\n",
    " 1. load docs\n",
    " 2. embed (sentance-transformer)\n",
    " 3. store vectors (FAISS)\n",
    " 4. retrieve\n",
    " 5. augment prompt\n",
    " 6. generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeea29c1",
   "metadata": {},
   "source": [
    "---Boring stuff\n",
    "\n",
    "1. Imports\n",
    "\n",
    "pip install sentence-transformers faiss-cpu numpy python-dotenv openai\n",
    "\n",
    "2. Load the .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e61b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY missing in environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2654572c",
   "metadata": {},
   "source": [
    "Docs\n",
    "\n",
    "Small scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83346d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"RAG stands for Retrieval-Augmented Generation.\",\n",
    "    \"In RAG, documents are embedded into a vector space.\",\n",
    "    \"FAISS is a library for efficient similarity search.\",\n",
    "    \"Large Language Models can hallucinate without grounding.\",\n",
    "    \"Embeddings capture semantic meaning of text.\",\n",
    "    \"Ronaldo has played for Real Madrid, Manchester United, Juventus, Sporting Lisbon, Portugal and Al Nassr\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896cd5b1",
   "metadata": {},
   "source": [
    "Chunking (no overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9144c9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RAG stands for Retrieval-Augmented Generation.', 'In RAG, documents are embedded into a vector space.', 'FAISS is a library for efficient similarity search.', 'Large Language Models can hallucinate without grounding.', 'Embeddings capture semantic meaning of text.', 'Ronaldo has played for Real Madrid, Manchester United, Juventus, Sporting Lisbon, Portugal and Al Nassr']\n"
     ]
    }
   ],
   "source": [
    "def chunk(text, chunk_size=50):\n",
    "  words = text.split()\n",
    "  return [\n",
    "      \" \".join(words[i:i+chunk_size])\n",
    "      for i in range(0,len(words), chunk_size)\n",
    "  ]\n",
    "chunks = []\n",
    "for docs in documents:\n",
    "  chunks.extend(chunk(docs))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e232f",
   "metadata": {},
   "source": [
    "With overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded28647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef chunk(text, chunk_size=5, overlap = 2):\\n    words = text.split()\\n    step = chunk_size - overlap\\n    return [\\n        \" \".join(words[i:i+chunk_size])\\n        for i in range(0,len(words),step)\\n        ]\\nchunks = []\\nfor doc in documents:\\n    chunks.extend(chunk(doc))\\nprint(chunks)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def chunk(text, chunk_size=5, overlap = 2):\n",
    "    words = text.split()\n",
    "    step = chunk_size - overlap\n",
    "    return [\n",
    "        \" \".join(words[i:i+chunk_size])\n",
    "        for i in range(0,len(words),step)\n",
    "        ]\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    chunks.extend(chunk(doc))\n",
    "print(chunks)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65485a07",
   "metadata": {},
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9b443a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satvikraghav/coding/RAG/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 792.89it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = embedder.encode(chunks)\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfc543",
   "metadata": {},
   "source": [
    "Vector Store (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36d8ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vectors: 6\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"total vectors:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c66aa8",
   "metadata": {},
   "source": [
    "Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc2c0232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Embeddings capture semantic meaning of text.\n",
      "- In RAG, documents are embedded into a vector space.\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query, k=2):\n",
    "    query_embedding = embedder.encode([query]).astype(\"float32\")\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "query = \"Why are embeddings useful?\"\n",
    "retrieved_chunks = retrieve(query)\n",
    "\n",
    "for c in retrieved_chunks:\n",
    "    print(\"-\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa07f8",
   "metadata": {},
   "source": [
    "LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38bdd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings capture the semantic meaning of text, allowing documents to be represented as vectors in a vector space and thus making it possible to compare and retrieve content based on meaning.\n"
     ]
    }
   ],
   "source": [
    "#Answering question related to content\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "context = \"\\n\".join(retrieved_chunks)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Answer the question using ONLY the context below.\n",
    "If the answer is not in the context, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query} #\"Why are embeddings useful?\"\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-3-nano-30b-a3b:free\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae074a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "## It says \"idk\" if question is not in the content\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    ")\n",
    "\n",
    "context = \"\\n\".join(retrieved_chunks)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Answer the question using ONLY the context below.\n",
    "If the answer is not in the context, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "\"How to make pizza\" #THE QUESTION\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"nvidia/nemotron-3-nano-30b-a3b:free\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf120dc3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
